{
  "answers": [
    {
      "number": "1.a",
      "question": "Describe different types of attributes that can be found in a dataset. Provide examples for each type.",
      "text": "Attributes - Data Attributes. These attributes define the different types of data in a dataset. A dataset enables different types of attributes. These are:\n1. **Nominal Attributes**: These are attributes that provide some kind information to just give the attributes based on the attributes. A dataset can be organized based on the nominal attributes. These attributes are used to classify the dataset based on some kind of characteristic. For example, 'Marks of students in an exam', 'Grades of a quality', 'Items'. These are the nominal attributes. The nominal attributes are attributes which give only general information.\nFor example, date, student's roll number, or some other attributes that provide the nominal data. These attributes are only to provide basic information. These attributes are also used for individual student. These are the roll numbers of the students, and Names of the table.\n2. **Binary Attributes**: These are attributes where the attributes of the dataset have either true or false values - 1/0. These attributes declare the attribute of dataset that is present or not. If the value is present, then it is 1, if absent, then it is 0. Therefore, binary attributes are used to denote 2 values. E.g., Male/Female, a student is present/absent, etc.\n3. **Numerical Attributes**: These are attributes where numerical data like integers are used to specify the data. Ex: No. of classes, etc.\n4. **Composite Attributes**: These are used to specify a dataset, where multiple data like for example to define a student dataset (e.g., roll no, alphanumeric, marks, income, etc.). These are to afford individual attributes.",
      "marks": 5,
      "remark": "Student correctly identified 4 types of attributes (Nominal, Binary, Numerical, Composite). Explanations are reasonable, but examples for Nominal are not ideal (marks/grades can be ordinal or numerical, roll number is usually nominal but lacks good classification example). Composite attribute explanation is also a bit generic. Full marks for identification, partial for clarity and examples."
    },
    {
      "number": "1.b",
      "question": "A healthcare organization wants to predict patient readmission rates. Given a dataset of patient records, describe the process of building a predictive model. Which data mining techniques would you employ? Elucidate.",
      "text": "If a healthcare organization wants to predict patient readmission rates, given a dataset consisting of patient medical records. We need to make sure and start a predictive model. We have to identify some useful information like the patient's age and medical records. After gathering the required information, we need to clean the data. This is known as 'data cleaning'. In this step, we will identify and remove bad/wrong data to clean the data. We need to remove redundancies and bad data to be eliminated from the data. Then we move into the actual data where we use algorithms and preprocessing techniques for different types. Then selected is the data. After all this, we need to find suitable methods which can perform data mining techniques like Naive Bayes classifier, Decision Tree, etc. to extract information from the dataset. After extracting information, we can derive some meaningful insights from the data. After the data is extracted, we need to validate insights in a dataset and we can infer novel insights based on the healthcare. Thus, the process for this is to develop a predictive model that helps to differentiate to predict patient readmission rates. Lastly, we can interpret the data to make an analytical process. In the healthcare record, these are the data mining techniques that I would employ.",
      "marks": 6,
      "remark": "Student outlines a logical process: data collection, cleaning, preprocessing, model building (mentioning Naive Bayes, Decision Trees), evaluation, and interpretation. This covers the main steps required for building a predictive model. The explanation is fairly good."
    },
    {
      "number": "2.a",
      "question": "What is data cleaning? List and explain the different ways of handling missing values.",
      "text": "Data Cleaning. Data cleaning is the most important part and the basic requirement of data mining. Data cleaning is the process in which we smooth the data, remove outliers, impute all the data from the dataset, and some necessities among these. Data cleaning helps to handle the raw data and make it suitable for the process of the data and predict the operations of the data. Therefore, it is useful to find and remove the most important errors in the data. This document describes each step of the data cleaning process in detail.\nData cleaning includes identifying the outliers. Outliers are the data points that exist outside the regular curve, maybe the data values. Outliers are to be removed because they cause bad data and lead to less accurate results. Therefore, we need to identify the outliers and eliminate them. We have to employ different techniques like normalization, etc. to smooth the outliers. We need to perform such techniques on the datasets to clean the data. After we find the outliers, we need to clean the remaining techniques like imputation, etc. These are the general data cleaning processes.\nAfter removing the outliers, we need to handle the missing data. If there are any missing data, then we need to fill them because otherwise, there will be inconsistent data and therefore, it may not be suitable to perform data mining operations. Therefore, to prevent such hindrances, we need to provide the true or fill in the missing data. To fill such data, we need to use imputation techniques to make a proper, decided (imputed?) data. We also need to smooth the data so that it is suitable to perform data mining techniques. To handle missing values, we can use techniques like min-max (normalization?), these are to take the maximum and minimum elements in a dataset and perform normalization to check the integrity of the data. We also use mean, median, and standard deviation to handle missing values.",
      "marks": 4,
      "remark": "The definition of data cleaning is good and mentions its importance. The student mentions 'imputation' for handling missing values and lists 'mean, median, and standard deviation' as techniques, which are correct. However, 'min-max normalization' is incorrectly described as a method for handling missing values, instead of a data transformation technique. The explanation of *different ways* of handling missing values is not fully elaborated with distinct methods (e.g., ignoring, manual fill, global constants, attribute mean/median, regression/inference)."
    },
    {
      "number": "2.b",
      "question": "Assume that the value of the income attribute are 2000,3000,4000,6000 and 10,000. The income has to be mapped to the range [0.0,1.0]. Do min-max normalization, z-score normalization and decimal scaling for income attribute.",
      "text": "Given: 2000, 3000, 4000, 6000, 10000. Range [0.0, 1.0].\n\nMin-Max Normalization:\nMin: 2000, Max: 10000. Range [0,1]\n(Value - Min) / (Max - Min)\nFor 6000: (6000 - 2000) / (10000 - 2000) = 4000 / 8000 = 0.5\nStudent's work: `Ma Movk nmmolooulien < mun : QO movx * 4OGCO (fao0o+ 1o00o) S000 1600 5 5 Numalbaken (eoo6 -l6oo) Czooo-(606) (Gooo-[6oo) + (6ooo-Iboo) t (Jopco-leoo 17000 2> [7000 0 . 68 3 Ju Omnamanc mmelhal%9n 35000 Aumm 0f valuo)`\n\nZ-score Normalization:\nMean = (2000 + 3000 + 4000 + 6000 + 10000) / 5 = 25000 / 5 = 5000\nStudent's work: `2-scg1_ Tomalsolion 2 SCe mecvn data 800 + 3Otloxt6030 +1000 bo00 Nomalealion bo00 )z 25000 5 O: & 1& 2-Koha _ Molmaleahcon`\n\nDecimal Scaling:\nMax absolute value = 10000. To scale it to [0,1], divide by 10^5 (k=5).\nStudent's work: `ecma soli telal Ancome LOOO+z000+ 4oo0 +6006 + J068 86000 ~dlamal Kolnd 8 6000 865826 105 1009ee 2 0, 86\" decmnoll htlid`",
      "marks": 2,
      "remark": "The student identifies the correct techniques but the calculations are largely incorrect and highly unreadable. For min-max, the formula is correct but the application is messed up. For z-score, the mean calculation is shown, but standard deviation and the final z-score calculation are completely missing/incorrect. For decimal scaling, the approach is wrong, and the calculations are unintelligible. Very minimal credit for identifying the methods and partially showing the correct mean for z-score."
    },
    {
      "number": "3.a",
      "question": "Compare and contrast three OLAP server architectures: MOLAP, ROLAP, and HOLAP. Discuss their advantages, disadvantages, and suitable use cases for each architecture.",
      "text": "OLAP stands for Online Analytical Processing. OLAP systems are used to perform the analytical processes on the data so that we can extract meaningful insights from the data and we determine and extract the different operations. These are the three different types of OLAP servers based on their architecture:\n1. Multidimensional OLAP Server (MOLAP)\n2. Relational OLAP Server (ROLAP)\n3. Hybrid OLAP Server (HOLAP)\nAll of these servers take some architecture in analytical processing such that each of them performs differently based on their technique.\n**MOLAP**: It also stands for Multidimensional Online Analytical Processing. It is the server which stores the data in most different dimensions and not only a single dimension. In MOLAP, the data is stored in the form of cubes where we perform different functions like slicing, dicing, extracting, etc. The data in MOLAP is structured data. In MOLAP, the dataset is stored in the form of cubes where we use the dataset to perform above mentioned operations and to fetch the individual data. It further states the bottom-up and we apply different techniques. It supports a first-dimensional database. It enables to understand the OLAP and techniques but it requires more skillful work. It is suitable where multiple dimensions in a data warehouse exist.\n**ROLAP**: Or Relational OLAP. It is OLAP in which the RDBMS (Relational Database Management System) is used to manage and we use these commercial databases for the operations. ROLAP is associated with RDBMS but the relational database operations can be performed at a very quick and structured pace and therefore, it is easy and fast to handle large datasets.\n**HOLAP**: Or Hybrid OLAP. It is a technique where the datasets are stored in a hierarchical process and we can analyze the datasets based on the hierarchy of the database. It is used to create a database for analytical processes. Therefore, it is much more complex than other OLAP architectures and is efficient.",
      "marks": 3,
      "remark": "Student correctly lists the three architectures and provides basic definitions for each. However, the answer lacks a clear comparison and contrast (advantages/disadvantages are not clearly distinguished or enumerated for each type, and specific use cases are not well-defined). The MOLAP description is somewhat generic. ROLAP's efficiency claim is partially true but lacks depth. HOLAP definition is too brief and misses its key hybrid nature. The answer doesn't meet the 'compare and contrast' requirement effectively."
    },
    {
      "number": "3.b",
      "question": "Suppose a data warehouse consists of three dimensions time, doctor and patient and two measures count and charge, where charge is the fee that a doctor charges a patient for a visit. i) Enumerate three classes of schemas that are popularly used for modelling data warehouses. ii) Draw a schema diagram for above data warehouse using schema classes listed in (i).",
      "text": "If a data warehouse consists of three dimensions: time, doctor, patient and two measures: count and charge. Where stored is the fact table and a doctor charges a patient for a visit.\nThe three classes of schemas usually used for modeling data warehouses are:\n1. **Star Schema**: It is the schema in which there is a central fact table and multiple dimension tables. All the dimension tables are directly connected to the main fact table. The star schema is a denormalized model showing very few tables because it limits the number of joins. Therefore, snowflake schema is not used because it is volatile. Similarly, dimension tables are connected to the main table, and also to sub-dimension tables of the dimension table, therefore, it is connected to the main table.\n2. **Fact Constellation**: If the schema has more than one fact table, then it is called Fact Constellation because the same tables are associated with two dimensional tables, and also connected with each other.\n\nii) Schema Diagram:\n[Diagram attempts to show a central table with \"Paleerd dclox_id chaxae_n\", and \"Juimm +Kme_% minles 6econd6 hans OtMENBIONAL TABLE\", \"tharge + chharae-n DIMENSIONAL TABLE\", \"DIMEe NBlowAl TABLE\". And \"count\".]\n\noloclox + Palexdt_i4; time _ id tcharae_n Cou tn\nPaleerd dclox_id chaxae_n\nJuimm +Kme_% minles 6econd6 hans OtMENBIONAL TABLE\ntharge + chharae-n\nDIMENSIONAL TABLE\nDIMEe NBlowAl TABLE\nJconslelLalfe wn sclama",
      "marks": 3,
      "remark": "i) Student correctly enumerates Star Schema and Fact Constellation. However, Snowflake schema, which is explicitly mentioned and contrasted, is not listed as a class itself, only as 'not used' for star schema description. Explanation for star schema is partial. Fact Constellation explanation is vague.\nii) The schema diagram is very poorly drawn and labeled. It does not clearly represent any of the named schemas with the given dimensions and measures in a standard way. The labels are highly illegible ('Palexdt_i4', 'Juimm +Kme_%', 'tharge + chharae-n'). No clear distinction of fact vs. dimension tables."
    },
    {
      "number": "4.a",
      "question": "Outline about multidimensional data model. Explain typical OLAP operations on multidimensional data.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    },
    {
      "number": "4.b",
      "question": "Explain the importance of indexing OLAP data and describe two indexing strategies that can be used to optimize query performance in an OLAP system.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    },
    {
      "number": "5.a",
      "question": "Write about decision tree induction with an algorithm for generating decision tree from training tuples.",
      "text": "Decision tree induction. Decision tree induction is a technique where we make use of decision tree to find patterns in data. In decision tree, the data is stored in such form as that will help in identifying different values. Decision trees are used to divide and group the patterns. Decision trees are generated so that we can recognize the patterns. Decision trees are usually made so that we can confirm them and we can perform operations on the datasets. Decision tree are also used to analyze the patterns. Decision trees are also used to create the dataset because the decision tree holds the dataset that has been generated. Decision tree is formed from bottom-up because decision tree is used to generate the decision tree with the help of an algorithm for decision tree induction. It generates its branches and rules to form and normalize the data. It aims to identify the required attributes.",
      "marks": 3,
      "remark": "The student provides a general description of decision tree induction, correctly identifying its purpose (finding patterns, dividing data). However, it lacks depth on the 'induction' aspect, such as criteria (information gain, Gini index) or how splits are determined. Crucially, it does not provide an algorithm for generating a decision tree from training tuples, which was a core part of the question. The mention of 'bottom-up' is unusual for decision tree induction, which is typically top-down (recursive partitioning)."
    },
    {
      "number": "5.b",
      "question": "Construct FP tree and find out frequent patterns for the following transaction data given in Table 1. Assume minimum support count as 3.",
      "text": "Transaction ID | Items\nT1            | {E, K, M, N, O, Y}\nT2            | {D, E, K, N, O, Y}\nT3            | {A, E, K, M}\nT4            | {C, K, M, U, Y}\nT5            | {C, E, I, K, O, O}\nMinimum support count: 3\n\nFrequent patterns: {E,K}, {K,O}, {K,M}, {K,Y}",
      "marks": 1,
      "remark": "The student correctly identifies the minimum support count. However, the FP-tree construction is not shown, which is a major part of the question. The listed 'frequent patterns' are only single pairs and appear to be incomplete or incorrectly derived without showing the FP-tree or conditional FP-trees. For example, K has a support of 5, E has 3, M has 3, O has 3, Y has 3, N has 2 (not frequent), C has 2 (not frequent), D has 1 (not frequent), A has 1 (not frequent), U has 1 (not frequent), I has 1 (not frequent). The frequent single items would be {E, K, M, O, Y}. The listed pairs are not exhaustive (e.g., {E,M} is frequent, {E,O} is frequent, {K,Y} is frequent). Minimal credit for identifying frequent items in some pairs without full method."
    },
    {
      "number": "6.a",
      "question": "Discuss effective methods that can be used to reduce the number of rules generated while still preserving most of the interesting rules.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    },
    {
      "number": "6.b",
      "question": "The following image shown in Figure 1 consists of training data from an employee database. The data have been generalized. For example, “31 … 35” for age represents the age range of 31 to 35. For a given row entry, count represents the number of data tuples having the values for department, status, age, and salary given in that row. Let status be the class-label attribute. i) Design a multilayer feed-forward neural network for the given data. Label the nodes in the input and output layers. ii) Using the multilayer feed-forward neural network obtained in (i), show the weight values after one iteration of the backpropagation algorithm, given the training instance “(sales, senior, 31 . . . 35, 46K . . . 50K)”. Indicate your initial weight values and biases and the learning rate used.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    },
    {
      "number": "7.a",
      "question": "Mention different types of data used for cluster analysis. List and explain the typical requirements of clustering in data mining.",
      "text": "There are different types of data used for cluster analysis. These are important steps in cluster analysis (clustering?) process in which related data is grouped together. The different types of data are:\n1. **Hierarchical data**\n2. **Grid-based data**\n3. **Model-based data**\n4. **Density-based data**\n5. **Statistical data**\nIn data-based clustering, the data is grouped together such that we identify the clusters. These are the two main clustering methods: K-means and K-medoids (mathematical method). In K-means method, the data is clustered such that the clusters are formed and we repeat until there is no change. K-medoids is a similar but less performed method. This is for finding values.\nHierarchical data: It is the data where agglomerative (we combine data from bottom-down) or divisive (we divide the data) methods are used. This is for density-based. The clusters are formed in grids (quads?) and have a different shape. Grid-based performs clustering where different types of data are clustered based on the different types of data. The data is of different types: hierarchical data, mixed data, numerical data and categorical data, etc.\nIn hierarchical data, the data is formed and is ranked in a system. So are the bottom-up or top-down approaches. Numerical data: where the data is in the form of integers, for example, values that are used for the count or number of data. Mixed data: is the data that cannot have some structure used to enter the general or overall something. Other types of data include: alphanumeric codes where both characters and numerals are present and range of data is present in the different data. For data clustering, the data can be structured and is different for each clustering technique mentioned above.",
      "marks": 3,
      "remark": "The student lists methods/approaches to clustering (hierarchical, grid-based, model-based, density-based, statistical) as 'types of data', which is a fundamental misunderstanding. The question asks for types of *data* (e.g., interval-scaled, binary, nominal, ordinal, ratio) used for clustering, not types of clustering *methods*. The subsequent explanations also mix data types with clustering methods. Credit is given for attempting to answer but with significant conceptual confusion."
    },
    {
      "number": "7.b",
      "question": "Consider the points A1(2, 10), A2(2, 5), A3(8, 4), B1(5, 8), B2(7, 5), B3(6, 4), C1(1, 2), C2(4, 9). Assume that Euclidean distance is used and the initial centers of the clusters are A1,B1 and C2. The distance function is Euclidean distance. Suppose initially we assign A1,B1, andC1as the center of each cluster, respectively. Use the k-means algorithm to show only i) The three cluster centers after the first round of execution. ii) The final three clusters.",
      "text": "Points: A1(2,10), A2(2,5), A3(8,4), B1(5,8), B2(7,5), B3(6,4), C1(1,2), C2(4,9)\nInitial Centroids (as used by student): C1=(2,10), C2=(5,8), C3=(1,2) (student labels them A1, B1, C1 as centroids)\n\n[Student's calculations for distances and reassigning points are shown in tables across two pages. These tables are messy and difficult to interpret, with many illegible entries. Some numbers appear to be distance calculations, but the assignment logic and centroid recalculations are not clear.]\n\nNew centroids after first round (student's 'Kenbu'): (6.5, 7.6), (6.95, 5.6), (1.5, 3.5)\n\nFinal Clusters (student's final line): `{A4 ,81,627 , 8 A?, 08, 83} , { A?,c13` (highly illegible and not interpretable as valid cluster groups).",
      "marks": 1,
      "remark": "The student identifies the points and attempts to list initial centroids (though uses C1 instead of C2 as per the question, which means their subsequent calculations would diverge). The distance calculation tables are extremely messy and unreadable, making it impossible to follow the assignment of points to clusters. The recalculation of centroids and the final clusters are either incorrect or unintelligible. Very minimal credit for setting up the problem but failing to execute the k-means algorithm correctly or legibly."
    },
    {
      "number": "8.a",
      "question": "How is the k-Medoids clustering method different from agglomerative and divisive clustering. Explain the method and develop an algorithm.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    },
    {
      "number": "8.b",
      "question": "Prove that in DBSCAN, the density-connectedness is an equivalence relation.",
      "text": "",
      "marks": 0,
      "remark": "Question not attempted."
    }
  ]
}