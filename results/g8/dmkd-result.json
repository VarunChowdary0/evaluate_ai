{
  "answers": [
    {
      "number": "1.a",
      "question": "Describe different types of attributes that can be found in a dataset. Provide examples for each type.",
      "text": "Types of attributes in a dataset; a dataset is a data source which consists of information/data about a specific class. Let us see the types of attributes found in a dataset:\n\n1. Nominal Attributes: These nominal attributes have fixed values, like examples: 1. User_Roles (User, admin, manager etc.), 2. Weather_Status (cloudy, sunny, rainy etc.), 3. Review_Status (Pending, Approved, Rejected).\nHere we see in the above example, these are fixed values that can be in the Nominal Attribute, and they are mainly used in classifications.\n\n2. Ordinal Attributes: The ordinal attribute has ordered values. It specifies the order in the dataset.\nExamples: 1. Grades (A+, A, B+, B, C+, C etc. -> A+ > A > B+ > B > C+ > C etc.), 2. Time_Slots (1:00 pm, 12:00 pm, 0:00 am etc.).\n\n3. Interval Attribute: Interval attributes are ordered attributes with no true zero, and each two consecutive values have a fixed interval between them.\nExamples: 1. Temperature (0°C, 1°C, 100°C etc.), 2. Time (0:00 AM, 12:00 PM, 6:50 AM etc.).\n\n4. Ratio Attributes: Ratio attributes are ordered and have true zeros. They could be numeric (value can be decimal value).\nExamples: 1. Marks ({0, 10, 12, 16, 18, 21, 25 etc.}), 2. Speed ({0.0, 0.01, 0.5, 0.65, 1.6 etc.}).\n\n5. Binary Attributes: As the name specifies, the Binary attribute consists of only binary values (Yes/No, 0/1, True/False).\nExamples: 1. Is_Eligible (Yes/No), 2. Is_Passed (True/False).\n\nLet us see a dataset with above attribute types:\nUser ID | Role      | Age | Marks | Is_Passed\n--------|-----------|-----|-------|-----------\n1       | Student   | 20  | 95    | T\n2       | Teacher   | 40  | 85    | T\n3       | Student   | 21  | 61    | F\n4       | Student   | 19  | 33    | F\n5       | Student   | 26  | 85    | T\n\n(Ordinal) (Nominal) (Ratio) (Ratio) (Binary)",
      "marks": 7,
      "remark": "Excellent answer. All types of attributes (Nominal, Ordinal, Interval, Ratio, Binary) are correctly identified, defined, and illustrated with clear examples. The summary table at the end is also well-done."
    },
    {
      "number": "1.b",
      "question": "A healthcare organization wants to predict patient readmission rates. Given a dataset of patient records, describe the process of building a predictive model. Which data mining techniques would you employ? Elucidate.",
      "text": "Given: a dataset of patient records. Aim: To build a predictive model to predict patient readmission rates.\n\nProcess of Building a Predictive Model:\n\n1. Data Cleaning: The dataset needs to be cleaned by identifying the inconsistent values and managing the outliers in the data. This will help reduce noise, improve data quality, and minimize data redundancy.\n\n2. Data Integration: In this process, the data from various data sources is integrated into a single dataset. This will help to decentralize the data, as the data is retrieved from various sources.\n\n3. Data Transformation: In this step, the data is transformed into a usable format to make the process easy to build the model. Here, techniques like Smoothing, Normalization, and Text Vectorization are used.\n\n4. Pattern Detection: By using data mining algorithms, the patterns that are interesting and relevant to the model are extracted. Irrelevant data is discarded.\n\n5. Pattern Evaluation: Model evaluation metrics are used (e.g., F2_Score, Accuracy, etc.) to check the accuracy and efficiency of the model.\n\n6. Model Representation: Front-end tools like dashboards, graphs are used to visualize the model and use it to predict future events.\n\nSo, I would employ these following data mining techniques to predict the patient's readmission probability based on the dataset provided by the healthcare organization:\n(i) Data Cleaning\n(ii) Data Integration\n(iii) Data Transformation\n(iv) Pattern Recognition\n(v) Model Evaluation\n(vi) Model Representation / Predict Future",
      "marks": 7,
      "remark": "A clear and comprehensive description of the KDD (Knowledge Discovery in Databases) process, which is appropriate for building a predictive model. The student correctly lists and explains each step. The explicit mention of data mining techniques to be employed reinforces the understanding."
    },
    {
      "number": "2.a",
      "question": "What is data cleaning? List and explain the different ways of handling missing values.",
      "text": "Data Cleaning:\nData cleaning is a very important step in data preparation. Data cleaning involves reducing noisy data in the dataset while identifying the outliers (any raw data which has inconsistent data with many missing values and redundant information). These missing and inconsistent data are handled in data cleaning. Data cleaning will improve the quality of the data and reduce redundancy.\n\n[Diagram: Raw Data -> Data Preprocessing (Cleaning: Identifying Outliers, Missing Values) -> Processed Data]\n\nSteps and Cleaning Tools:\nPerform descriptive analysis using tools like pandas in Python and check the missing values in the dataset. `df.isna().sum()` gets the missing values. `df.fillna()` can be used to fill the missing values.\n\nHandling Missing Values:\nMissing values cause issues in the data, so they need to be handled.\n\n1. Replacing with Mean: We replace the missing values with the mean of the total values in that column.\n\n2. Replace with Median/Mode (interpreted from 'Cuolaf value'): We will calculate the median/mode value and put it in place of the missing value.\n\n3. Remove (Ignore) the tuple: If there is a value missing in that row (tuple), we just ignore that tuple.\n\nAnd there are other methods to handle the missing values in the dataset, like using a prediction model to predict the missing value.",
      "marks": 7,
      "remark": "The definition of data cleaning is accurate, covering noise, outliers, inconsistent values, and redundancy. The explanation of handling missing values (mean, median/mode, removal, prediction model) is correct and well-described. The mention of Python pandas functions is a good practical addition."
    },
    {
      "number": "2.b",
      "question": "Assume that the value of the income attribute are 2000,3000,4000,6000 and 10,000. The income has to be mapped to the range [0.0,1.0]. Do min-max normalization, z-score normalization and decimal scaling for income attribute.",
      "text": "Given: The values of the income attribute: 2000, 3000, 4000, 6000, 10000.\nTarget Range: [0.0, 1.0]\n\nLet us perform the Normalization methods specified:\n\n1. Min-Max Normalization:\nIn Min-Max Normalization, the data will be transformed between new_min_value and new_max_value.\nFormula: `v' = (v - min_A) / (max_A - min_A) * (new_max_A - new_min_A) + new_min_A`\nHere, `new_min_A = 0`, `new_max_A = 1`\nSo, `v' = (v - min_A) / (max_A - min_A)`\n\nMin_value = 2000, Max_value = 10000.\n\nCalculations:\n- For 2000: (2000 - 2000) / (10000 - 2000) = 0 / 8000 = 0.  (Student wrote 0.15)\n- For 3000: (3000 - 2000) / (10000 - 2000) = 1000 / 8000 = 0.125. (Student wrote 0.15)\n- For 4000: (4000 - 2000) / (10000 - 2000) = 2000 / 8000 = 0.25. (Student wrote 0.25)\n- For 6000: (6000 - 2000) / (10000 - 2000) = 4000 / 8000 = 0.5. (Student wrote 0.5)\n- For 10000: (10000 - 2000) / (10000 - 2000) = 8000 / 8000 = 1.0. (Student wrote 1.0)\n\nStudent's Min Max Normalized Values: {0.15, 0.15, 0.25, 0.5, 1.0}\n\n2. Z-Score Normalization:\nFormula: `v' = (v - mean_A) / std_dev_A`\n\nMean_A:\n(2000 + 3000 + 4000 + 6000 + 10000) / 5 = 25000 / 5 = 5000. (Correct)\n\nStd_dev_A:\n`sqrt[ ((2000-5000)^2 + (3000-5000)^2 + (4000-5000)^2 + (6000-5000)^2 + (10000-5000)^2) / 5 ]`\n`sqrt[ (9000000 + 4000000 + 1000000 + 1000000 + 25000000) / 5 ]`\n`sqrt[ 40000000 / 5 ] = sqrt[ 8000000 ] = 2828.427`\n(Student calculated `Sd = 536.6` which is incorrect.)\n\nCalculations (based on student's incorrect `Sd = 536.6`):\n- For 2000: (2000 - 5000) / 536.6 = -3000 / 536.6 = -5.59. (Student wrote -5.6)\n- For 3000: (3000 - 5000) / 536.6 = -2000 / 536.6 = -3.72. (Student wrote -3.72)\n- For 4000: (4000 - 5000) / 536.6 = -1000 / 536.6 = -1.86. (Student wrote -1.86)\n- For 6000: (6000 - 5000) / 536.6 = 1000 / 536.6 = 1.86. (Student wrote 1.86)\n- For 10000: (10000 - 5000) / 536.6 = 5000 / 536.6 = 9.319. (Student wrote 9.31)\n\nStudent's Z-Score Normalized Values: {-5.6, -3.72, -1.86, 1.86, 9.31}\n\n3. Decimal Scaling:\nFormula: `v' = v / 10^j`, where j is the smallest integer such that `max(|v'|) < 1`.\n\nMax value = 10000. To make it less than 1, we need to divide by `10^5 = 100000`. So, `j = 5`.\n(Student used `j=4` implicitly by dividing by 10000.)\n\nCalculations:\n- For 2000: 2000 / 100000 = 0.02. (Student wrote 0.2)\n- For 3000: 3000 / 100000 = 0.03. (Student wrote 0.3)\n- For 4000: 4000 / 100000 = 0.04. (Student wrote 0.4)\n- For 6000: 6000 / 100000 = 0.06. (Student wrote 0.6)\n- For 10000: 10000 / 100000 = 0.1. (Student wrote 1.0)\n\nStudent's Decimal Scaled Values: {0.2, 0.3, 0.4, 0.6, 1.0}",
      "marks": 2,
      "remark": "Formulas for all three normalizations are correctly stated. However, calculations for Min-Max (first two values), Z-score (standard deviation calculation), and Decimal Scaling (incorrect power of 10 used) are mostly incorrect due to numerical errors. Only 2 marks awarded for showing correct formulas and attempting the steps."
    },
    {
      "number": "3.a",
      "question": "Compare and contrast three OLAP server architectures: MOLAP, ROLAP, and HOLAP. Discuss their advantages, disadvantages, and suitable use cases for each architecture.",
      "text": "Online Analytic Processing (OLAP):\nOLAP servers are used as a bridge between the data warehouse and the reporting tools. The OLAP servers will perform the analytical operations on the data stored in the storage where the data marts, operational repositories, and data warehouses are located. And the analyzed information is ready to be used by the front-end body to visualize the information.\nThere are three Online Analytic Processing server architectures; they are:\n\n1. Relational Online Analytic Processing (ROLAP):\nThe ROLAP servers are built upon the Relational Database System, which consists of normalized data like tables, attributes, and relations. And high-level complex SQL (Structured Query Language) queries are used to perform the operations on ROLAP Server.\nAdvantages:\n- Easy to implement\n- Suitable for normalized data warehouse\nDisadvantages:\n- Less efficient\n- More storage\n- Complex queries for SQL\n\n2. Multidimensional Online Analytic Processing (MOLAP):\nThe MOLAP servers are flexible and are used to perform operations on multidimensional data. Querying the data is stored in the form of cubes (data cubes).\nAdvantages:\n- Faster query performance\n- Suitable for multidimensional data\nDisadvantages:\n- Inefficient for small data (should be 'efficient for small data' or 'inefficient for very large sparse data')\n- Hard to handle, unclear dimensions\n- DMQL (Data Mining Query Language) (This seems out of place as DMQL is typically for data mining, not directly an OLAP disadvantage.)\n\n3. Hybrid Online Analytic Processing (HOLAP):\nHOLAP combines the features of ROLAP and MOLAP. i.e., the data can be stored in the normalized relational form like tables and relations, or data could be multidimensional and stored in data cubes.\nAdvantages:\n- HOLAP is effective and efficient\n- Easy for small and heavy data\n- Provides accurate results\nDisadvantages:\n- More complex to implement and use\n- Need experienced staff to maintain the server\n\nOLAP servers can perform these operations: Roll-up, Slice, Pivot/Rotate.",
      "marks": 7,
      "remark": "The answer provides a good definition of OLAP. It correctly describes ROLAP, MOLAP, and HOLAP, including their advantages and disadvantages. The points are relevant and mostly accurate. The mention of OLAP operations at the end is a good addition. There's a minor inaccuracy in MOLAP disadvantage ('inefficient for small data' should be more like 'scalability issues for very large sparse data')."
    },
    {
      "number": "3.b",
      "question": "Suppose a data warehouse consists of three dimensions time, doctor and patient and two measures count and charge, where charge is the fee that a doctor charges a patient for a visit.\ni) Enumerate three classes of schemas that are popularly used for modelling data warehouses.\nii) Draw a schema diagram for above data warehouse using schema classes listed in (i).",
      "text": "Given: A data warehouse with dimensions (Time, Doctor, Patient) and measures (Count, Charge).\n\ni) Three classes of schemas that are popularly used for modeling data warehouses:\n1. Star Schema\n2. Snowflake Schema\n3. Galaxy Schema (or Fact Constellation Schema)\n\nStar Schema:\nIn Star Schema, there is one fact table and multiple dimension tables related to the fact table.\nFact Table: The table that stores the measure values.\nDimension Table: The table that stores descriptive data.\n\nSnowflake Schema:\nSnowflake Schema is an extension of Star Schema that has multiple sub-dimensions for each dimension, related to the fact table. It does not have only a single fact table.\n\nGalaxy Schema:\nThe Galaxy or Fact Constellation Schema is the complex form of multiple distinct Star Schemas. In the Galaxy Schema, there are two or more fact tables that share their dimension tables.\n\nii) Schema diagrams for the given data warehouse:\n\nStar Schema:\n\n          [Doctor Dimension]\n           (doctor id, name, designation)\n                 |\n                 |\n           [Fact Table]\n (doctor id, time id, patient id, count, charge)\n         /     |     \\\n        /      |      \\\n       /       |       \\\n[Patient Dimension]   [Time Dimension]\n (patient id, name, age)  (time id, hour, min, second)\n\n\nSnowflake Schema:\n(The diagram provided by the student is messy and difficult to accurately reproduce or interpret here. It attempts to show sub-dimensions for 'doctor' like 'specialty' or 'category'.)\n\nGalaxy Schema:\n(The diagram provided by the student attempts to show two fact tables sharing dimensions, but the fact table content is incomplete/inconsistent.)\n\n            [Time Dimension]\n            (time id, timestamp)\n           /                \\\n          /                  \\\n[Fact Table 1]          [Fact Table 2]\n(doctor id, time id, charge)   (patient id, time id, charge)\n          |                     |\n    [Doctor Dimension]     [Patient Dimension]\n    (doctor id, name)        (patient id, name)\n",
      "marks": 4,
      "remark": "i) Enumeration of schemas is correct (Star, Snowflake, Galaxy) and definitions are good.\nii) Star Schema diagram is clear and mostly correct based on the problem statement. Snowflake diagram is too messy and unclear to evaluate properly, depicting an unclear attempt at normalization. Galaxy schema diagram is conceptually correct (multiple fact tables sharing dimensions) but the specific attributes in the fact tables are incomplete and inconsistent with the problem's measures (only 'charge' mentioned, 'count' is missing). Awarded 3 marks for correct enumeration/definitions and 1 mark for a clear Star schema, with deductions for the unclear/inaccurate Snowflake and Galaxy diagrams."
    },
    {
      "number": "5.b",
      "question": "Construct FP tree and find out frequent patterns for the following transaction data given in Table 1. Assume minimum support count as 3.",
      "text": "Given: Minimum Support Threshold = 3\n\nTransaction Data:\nT1: {E, K, M, N, O, Y}\nT2: {D, E, K, N, O, Y}\nT3: {A, E, K, M}\nT4: {C, K, M, U, Y}\nT5: {C, E, I, K, O}\n\nStep 1: Calculate support count for each item and filter out infrequent items (L1):\nItem | Count\n-----|------\nA    | 1\nC    | 2\nD    | 1\nE    | 4 (Frequent)\nI    | 1\nK    | 5 (Frequent)\nM    | 3 (Frequent)\nN    | 2\nO    | 3 (Frequent)\nU    | 1\nY    | 3 (Frequent)\n\nFrequent 1-itemsets (Support >= 3): E:4, K:5, M:3, O:3, Y:3\n\nStep 2: Order frequent items by decreasing support count:\nK(5), E(4), M(3), O(3), Y(3)\n\nStep 3: Build the FP-Tree:\n(Student's FP-Tree diagram is not provided in a clear, standard graphical format. Instead, there's a textual representation that seems to list conditional pattern bases and head table links, but not the actual tree structure with nodes and counts.)\n\nTransactions reordered based on frequent item order (K, E, M, O, Y):\nT1: {K, E, M, O, Y}\nT2: {K, E, O, Y}\nT3: {K, E, M}\nT4: {K, M, Y}\nT5: {K, E, O}\n\n(Student's textual representation of FP-Tree / Frequent Patterns is unclear, appears to be head table links and possibly conditional pattern bases, but not a final list of frequent patterns.)\n\nFrequent Patterns:\n(Student listed K, E, M, O, Y with their counts again, which are 1-itemset frequent patterns, not the higher-order patterns derived from the FP-tree.)\nK: 5\nE: 4\nM: 3\nO: 3\nY: 3",
      "marks": 2,
      "remark": "The student correctly identified the frequent 1-itemsets and ordered them by support count. However, the FP-Tree itself is not drawn or clearly represented. The 'Frequent Patterns' section at the end only lists the 1-itemset frequent items, not the higher-order frequent patterns derived from the FP-Tree. Significant deductions for the missing FP-Tree and incomplete frequent pattern extraction."
    },
    {
      "number": "7.b",
      "question": "Consider the points A1(2, 10), A2(2, 5), A3(8, 4), B1(5, 8), B2(7, 5), B3(6, 4), C1(1, 2), C2(4, 9). Assume that Euclidean distance is used and the initial centers of the clusters are A1,B1 and C2. The distance function is Euclidean distance. Suppose initially we assign A1,B1, andC1as the center of each cluster, respectively. Use the k-means algorithm to show only\ni) The three cluster centers after the first round of execution.\nii) The final three clusters.",
      "text": "Given data points: A1(2, 10), A2(2, 5), A3(8, 4), B1(5, 8), B2(7, 5), B3(6, 4), C1(1, 2), C2(4, 9).\nGiven initial centers of the clusters are A1, B1, and C2.\nInitial Centroids:\nC1_initial = A1(2,10)\nC2_initial = B1(5,8)\nC3_initial = C2(4,9)\n\n(Student's initial centroids for calculation were incorrect: C1_initial (2,2), C2_initial (6.5,6.5), C3_initial (2,2).)\n\nCalculations based on student's incorrect initial centroids:\n\nDistances to Centroid 1 (2,2):\n- A1(2,10): `sqrt((2-2)^2 + (10-2)^2) = sqrt(0 + 64) = 8`\n- A2(2,5): `sqrt((2-2)^2 + (5-2)^2) = sqrt(0 + 9) = 3`\n- A3(8,4): `sqrt((8-2)^2 + (4-2)^2) = sqrt(36 + 4) = sqrt(40) = 6.32`\n- B1(5,8): `sqrt((5-2)^2 + (8-2)^2) = sqrt(9 + 36) = sqrt(45) = 6.71`\n- B2(7,5): `sqrt((7-2)^2 + (5-2)^2) = sqrt(25 + 9) = sqrt(34) = 5.83`\n- B3(6,4): `sqrt((6-2)^2 + (4-2)^2) = sqrt(16 + 4) = sqrt(20) = 4.47`\n- C1(1,2): `sqrt((1-2)^2 + (2-2)^2) = sqrt(1 + 0) = 1`\n- C2(4,9): `sqrt((4-2)^2 + (9-2)^2) = sqrt(4 + 49) = sqrt(53) = 7.28`\n\n(Student's distance calculations are based on their own wrong initial centroids, and many values are also incorrect even with those centroids. For example, for A1(2,10) to (2,2), student lists '2.6' (wrong, should be 8); for A2(2,5) to (2,2), student lists '4.2' (wrong, should be 3).)\n\ni) The three cluster centers after the first round of execution:\n(Student lists (2,2), (6.5,6.5), (2,2) again as 'new center' which are their assumed initial centers, not computed centers after one round.)\n\nii) The final three clusters:\n(Student's final clusters are based on their incorrect calculations.)\n\nCluster 1: {A1, C1} (incorrect)\nCluster 2: {A2, B1, B2} (incorrect)\nCluster 3: {A3, B3, C2} (incorrect)\n",
      "marks": 0,
      "remark": "The student failed to use the given initial cluster centers (A1, B1, C2) from the question and instead used completely different incorrect centroids (2,2), (6.5,6.5), (2,2). All subsequent distance calculations, cluster assignments, and new centroid computations are therefore incorrect. No marks awarded."
    }
  ]
}